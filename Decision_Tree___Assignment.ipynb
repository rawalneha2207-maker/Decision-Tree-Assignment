{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Question 1: What is a Decision Tree, and how does it work in the context of\n",
        "classification?"
      ],
      "metadata": {
        "id": "LC-Qd7WvKMob"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A Decision Tree is a supervised machine learning algorithm used for classification and regression. In the context of classification, it predicts a class label by learning a series of decision rules from the training data.\n",
        "\n",
        "How a Decision Tree Works in Classification\n",
        "\n",
        "Tree Structure\n",
        "\n",
        "The model is structured like a tree:\n",
        "\n",
        "Root node: represents the entire dataset.\n",
        "\n",
        "Internal nodes: represent decisions based on feature values.\n",
        "\n",
        "Branches: represent outcomes of those decisions.\n",
        "\n",
        "Leaf nodes: represent the final class labels.\n",
        "\n",
        "Splitting the Data\n",
        "\n",
        "At each node, the algorithm selects the best feature to split the data.\n",
        "\n",
        "The goal is to create subsets that are as pure as possible (i.e., contain data points from mostly one class).\n",
        "\n",
        "Common criteria used to choose splits:\n",
        "\n",
        "Gini Impurity\n",
        "\n",
        "Entropy (Information Gain)\n",
        "\n",
        "Recursive Process\n",
        "\n",
        "The splitting process continues recursively on each subset.\n",
        "\n",
        "It stops when:\n",
        "\n",
        "All data points in a node belong to the same class,\n",
        "\n",
        "A maximum tree depth is reached,\n",
        "\n",
        "Or no further improvement is possible.\n",
        "\n",
        "Making Predictions\n",
        "\n",
        "To classify a new data point, the tree starts at the root and follows the decision rules based on the feature values.\n",
        "\n",
        "The traversal ends at a leaf node, which gives the predicted class.\n",
        "\n",
        "Example\n",
        "\n",
        "If a decision tree is used to classify emails as Spam or Not Spam, it may ask questions like:\n",
        "\n",
        "‚ÄúDoes the email contain the word ‚Äòfree‚Äô?‚Äù\n",
        "\n",
        "‚ÄúIs the sender known?‚Äù\n",
        "\n",
        "Based on the answers, the tree follows a path and finally classifies the email.\n",
        "\n",
        "Key Advantages\n",
        "\n",
        "Easy to understand and interpret\n",
        "\n",
        "Handles both numerical and categorical data\n",
        "\n",
        "Requires little data preprocessing\n",
        "\n",
        "Key Limitations\n",
        "\n",
        "Prone to overfitting\n",
        "\n",
        "Small changes in data can lead to different trees\n",
        "\n",
        "In summary, a decision tree for classification works by learning a hierarchy of if‚Äìelse rules that split the data into increasingly pure groups until a final class decision is made."
      ],
      "metadata": {
        "id": "kjRs7OH7KvYX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 2: Explain the concepts of Gini Impurity and Entropy as impurity measures.\n",
        "How do they impact the splits in a Decision Tree?"
      ],
      "metadata": {
        "id": "zZ6TbMSJMlUz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gini Impurity and Entropy are measures used in Decision Trees to evaluate how impure (mixed) the classes are at a node. They help the algorithm decide where and how to split the data.\n",
        "\n",
        "1. Gini Impurity\n",
        "Definition\n",
        "\n",
        "Gini Impurity measures the probability that a randomly chosen data point would be misclassified if it were labeled according to the class distribution in the node.\n",
        "\n",
        "Formula\n",
        "Gini\n",
        "=\n",
        "1\n",
        "‚àí\n",
        "‚àë\n",
        "ùëñ\n",
        "=\n",
        "1\n",
        "ùê∂\n",
        "ùëù\n",
        "ùëñ\n",
        "2\n",
        "Gini=1‚àí\n",
        "i=1\n",
        "‚àë\n",
        "C\n",
        "\t‚Äã\n",
        "\n",
        "p\n",
        "i\n",
        "2\n",
        "\t‚Äã\n",
        "\n",
        "\n",
        "where:\n",
        "\n",
        "ùê∂\n",
        "C = number of classes\n",
        "\n",
        "ùëù\n",
        "ùëñ\n",
        "p\n",
        "i\n",
        "\t‚Äã\n",
        "\n",
        " = proportion of samples belonging to class\n",
        "ùëñ\n",
        "i\n",
        "\n",
        "Interpretation\n",
        "\n",
        "Gini = 0 ‚Üí Node is pure (all samples belong to one class)\n",
        "\n",
        "Higher Gini ‚Üí More class mixing\n",
        "\n",
        "Example\n",
        "\n",
        "If a node has:\n",
        "\n",
        "70% Class A, 30% Class B\n",
        "\n",
        "Gini\n",
        "=\n",
        "1\n",
        "‚àí\n",
        "(\n",
        "0.7\n",
        "2\n",
        "+\n",
        "0.3\n",
        "2\n",
        ")\n",
        "=\n",
        "0.42\n",
        "Gini=1‚àí(0.7\n",
        "2\n",
        "+0.3\n",
        "2\n",
        ")=0.42\n",
        "2. Entropy\n",
        "Definition\n",
        "\n",
        "Entropy measures the uncertainty or randomness in the class labels at a node.\n",
        "\n",
        "Formula\n",
        "Entropy\n",
        "=\n",
        "‚àí\n",
        "‚àë\n",
        "ùëñ\n",
        "=\n",
        "1\n",
        "ùê∂\n",
        "ùëù\n",
        "ùëñ\n",
        "log\n",
        "‚Å°\n",
        "2\n",
        "(\n",
        "ùëù\n",
        "ùëñ\n",
        ")\n",
        "Entropy=‚àí\n",
        "i=1\n",
        "‚àë\n",
        "C\n",
        "\t‚Äã\n",
        "\n",
        "p\n",
        "i\n",
        "\t‚Äã\n",
        "\n",
        "log\n",
        "2\n",
        "\t‚Äã\n",
        "\n",
        "(p\n",
        "i\n",
        "\t‚Äã\n",
        "\n",
        ")\n",
        "Interpretation\n",
        "\n",
        "Entropy = 0 ‚Üí Node is perfectly pure\n",
        "\n",
        "Maximum entropy ‚Üí Classes are evenly mixed\n",
        "\n",
        "Example\n",
        "\n",
        "For the same node:\n",
        "\n",
        "70% Class A, 30% Class B\n",
        "\n",
        "Entropy\n",
        "=\n",
        "‚àí\n",
        "(\n",
        "0.7\n",
        "log\n",
        "‚Å°\n",
        "2\n",
        "0.7\n",
        "+\n",
        "0.3\n",
        "log\n",
        "‚Å°\n",
        "2\n",
        "0.3\n",
        ")\n",
        "‚âà\n",
        "0.88\n",
        "Entropy=‚àí(0.7log\n",
        "2\n",
        "\t‚Äã\n",
        "\n",
        "0.7+0.3log\n",
        "2\n",
        "\t‚Äã\n",
        "\n",
        "0.3)‚âà0.88\n",
        "3. Impact on Decision Tree Splits\n",
        "\n",
        "At each node, the decision tree:\n",
        "\n",
        "Calculates the impurity (Gini or Entropy) before the split\n",
        "\n",
        "Calculates the impurity after the split\n",
        "\n",
        "Chooses the split that:\n",
        "\n",
        "Minimizes Gini Impurity, or\n",
        "\n",
        "Maximizes Information Gain (reduction in entropy)\n",
        "\n",
        "Information Gain (using Entropy)\n",
        "Information Gain\n",
        "=\n",
        "Entropy(parent)\n",
        "‚àí\n",
        "‚àë\n",
        "(\n",
        "ùëõ\n",
        "child\n",
        "ùëõ\n",
        "parent\n",
        "√ó\n",
        "Entropy(child)\n",
        ")\n",
        "Information Gain=Entropy(parent)‚àí‚àë(\n",
        "n\n",
        "parent\n",
        "\t‚Äã\n",
        "\n",
        "n\n",
        "child\n",
        "\t‚Äã\n",
        "\n",
        "\t‚Äã\n",
        "\n",
        "√óEntropy(child))\n",
        "\n",
        "4. Gini vs Entropy (Comparison)\n",
        "\n",
        "| Aspect      | Gini Impurity                       | Entropy                            |\n",
        "| ----------- | ----------------------------------- | ---------------------------------- |\n",
        "| Concept     | Misclassification probability       | Measure of uncertainty             |\n",
        "| Computation | Faster                              | Slightly slower (log calculations) |\n",
        "| Preference  | Often used in practice (e.g., CART) | More theoretical                   |\n",
        "| Result      | Similar splits in most cases        | Similar splits in most cases       |\n",
        "Summary\n",
        "\n",
        "Gini Impurity and Entropy quantify how mixed the classes are at a node.\n",
        "\n",
        "They guide the decision tree to choose splits that create purer child nodes.\n",
        "\n",
        "Better splits ‚Üí clearer decision rules ‚Üí more accurate classification.\n",
        "\n",
        "Both measures usually lead to very similar trees, but Gini is often preferred for efficiency."
      ],
      "metadata": {
        "id": "70yft_g1Nng8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 3: What is the difference between Pre-Pruning and Post-Pruning in Decision\n",
        "Trees? Give one practical advantage of using each.\n"
      ],
      "metadata": {
        "id": "612PFIlJN4Z8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pre-Pruning and Post-Pruning are techniques used to control the growth of a Decision Tree and reduce overfitting. The key difference lies in when the pruning is applied.\n",
        "\n",
        "1. Pre-Pruning (Early Stopping)\n",
        "Definition\n",
        "\n",
        "Pre-pruning stops the tree while it is being built, before it fully fits the training data.\n",
        "\n",
        "How it Works\n",
        "\n",
        "The tree growth is halted based on predefined conditions such as:\n",
        "\n",
        "Maximum tree depth (max_depth)\n",
        "\n",
        "Minimum number of samples required to split a node (min_samples_split)\n",
        "\n",
        "Minimum samples required at a leaf (min_samples_leaf)\n",
        "\n",
        "Minimum impurity decrease\n",
        "\n",
        "If a split does not satisfy these conditions, it is not performed.\n",
        "\n",
        "Practical Advantage\n",
        "\n",
        "‚úÖ Faster training and simpler models\n",
        "\n",
        "The tree is smaller, easier to interpret, and requires less computation.\n",
        "\n",
        "Useful when working with large datasets or limited resources.\n",
        "\n",
        "Limitation\n",
        "\n",
        "‚ùå May stop too early and underfit the data.\n",
        "\n",
        "2. Post-Pruning (Pruning After Training)\n",
        "Definition\n",
        "\n",
        "Post-pruning allows the tree to grow fully and then removes branches that do not improve performance.\n",
        "\n",
        "How it Works\n",
        "\n",
        "A fully grown tree is created.\n",
        "\n",
        "Subtrees that contribute little to predictive accuracy are removed.\n",
        "\n",
        "Common approach: Cost-Complexity Pruning (CCP).\n",
        "\n",
        "Practical Advantage\n",
        "\n",
        "‚úÖ Better generalization\n",
        "\n",
        "Since the full structure is considered first, important splits are less likely to be missed.\n",
        "\n",
        "Often results in higher accuracy on unseen (test) data.\n",
        "\n",
        "Limitation\n",
        "\n",
        "‚ùå More computationally expensive.\n",
        "\n",
        "3. Key Differences Summary\n",
        "\n",
        "| Aspect         | Pre-Pruning                 | Post-Pruning                  |\n",
        "| -------------- | --------------------------- | ----------------------------- |\n",
        "| When applied   | During tree construction    | After tree is fully built     |\n",
        "| Tree size      | Smaller from the start      | Large initially, then reduced |\n",
        "| Risk           | Underfitting                | Overfitting before pruning    |\n",
        "| Computation    | Less                        | More                          |\n",
        "| Model accuracy | May miss important patterns | Often better generalization   |\n",
        "\n",
        "\n",
        "Final Summary\n",
        "\n",
        "Pre-pruning controls complexity early and is efficient.\n",
        "\n",
        "Post-pruning refines a fully grown tree for better performance.\n",
        "\n",
        "In practice, a combination of both often yields the best results."
      ],
      "metadata": {
        "id": "AXXxCg-cN9jr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 4: What is Information Gain in Decision Trees, and why is it important for\n",
        "choosing the best split?"
      ],
      "metadata": {
        "id": "hul7eIrQOVTH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Information Gain is a metric used in Decision Trees (especially with entropy) to measure how much a feature reduces uncertainty about the class labels after a split. It helps the algorithm decide which feature makes the best split at each node.\n",
        "\n",
        "Definition of Information Gain\n",
        "\n",
        "Information Gain (IG) is the reduction in entropy achieved by splitting a dataset based on a particular feature.\n",
        "\n",
        "Information Gain\n",
        "=\n",
        "Entropy(parent)\n",
        "‚àí\n",
        "Weighted Average Entropy(children)\n",
        "Information Gain=Entropy(parent)‚àíWeighted Average Entropy(children)\n",
        "Components Explained\n",
        "\n",
        "Entropy (Parent Node)\n",
        "Measures the impurity or randomness in the class labels before the split.\n",
        "\n",
        "Entropy (Child Nodes)\n",
        "Measures impurity after splitting the data using a specific feature.\n",
        "\n",
        "Weighted Average\n",
        "Each child node‚Äôs entropy is weighted by the proportion of samples it contains.\n",
        "\n",
        "Formula\n",
        "IG\n",
        "=\n",
        "ùêª\n",
        "(\n",
        "ùëÜ\n",
        ")\n",
        "‚àí\n",
        "‚àë\n",
        "ùëñ\n",
        "=\n",
        "1\n",
        "ùëõ\n",
        "‚à£\n",
        "ùëÜ\n",
        "ùëñ\n",
        "‚à£\n",
        "‚à£\n",
        "ùëÜ\n",
        "‚à£\n",
        "√ó\n",
        "ùêª\n",
        "(\n",
        "ùëÜ\n",
        "ùëñ\n",
        ")\n",
        "IG=H(S)‚àí\n",
        "i=1\n",
        "‚àë\n",
        "n\n",
        "\t‚Äã\n",
        "\n",
        "‚à£S‚à£\n",
        "‚à£S\n",
        "i\n",
        "\t‚Äã\n",
        "\n",
        "‚à£\n",
        "\t‚Äã\n",
        "\n",
        "√óH(S\n",
        "i\n",
        "\t‚Äã\n",
        "\n",
        ")\n",
        "\n",
        "Where:\n",
        "\n",
        "ùêª\n",
        "(\n",
        "ùëÜ\n",
        ")\n",
        "H(S) = Entropy of the parent dataset\n",
        "\n",
        "ùëÜ\n",
        "ùëñ\n",
        "S\n",
        "i\n",
        "\t‚Äã\n",
        "\n",
        " = Subset after the split\n",
        "\n",
        "‚à£\n",
        "ùëÜ\n",
        "ùëñ\n",
        "‚à£\n",
        "‚à£S\n",
        "i\n",
        "\t‚Äã\n",
        "\n",
        "‚à£ = Number of samples in subset\n",
        "\n",
        "‚à£\n",
        "ùëÜ\n",
        "‚à£\n",
        "‚à£S‚à£ = Total number of samples\n",
        "\n",
        "Why Information Gain Is Important\n",
        "1. Helps Choose the Best Split\n",
        "\n",
        "The feature with the highest Information Gain produces the purest child nodes.\n",
        "\n",
        "This leads to clearer decision rules.\n",
        "\n",
        "2. Reduces Uncertainty\n",
        "\n",
        "High Information Gain means a large drop in entropy, so the model becomes more confident in predictions.\n",
        "\n",
        "3. Improves Model Accuracy\n",
        "\n",
        "Better splits early in the tree often result in higher classification accuracy and simpler trees.\n",
        "\n",
        "4. Makes the Tree Efficient\n",
        "\n",
        "Choosing optimal splits early reduces unnecessary depth and complexity.\n",
        "\n",
        "Example (Intuitive)\n",
        "\n",
        "In a Spam vs Not Spam classifier:\n",
        "\n",
        "If splitting on ‚ÄúContains word free‚Äù greatly separates spam from non-spam emails,\n",
        "\n",
        "Entropy drops significantly,\n",
        "\n",
        "Information Gain is high,\n",
        "‚Üí This feature is chosen for the split.\n",
        "\n",
        "Key Notes\n",
        "\n",
        "Information Gain is mainly used with Entropy (ID3, C4.5 algorithms).\n",
        "\n",
        "It can be biased toward features with many unique values.\n",
        "\n",
        "Alternatives like Gain Ratio address this limitation.\n",
        "\n",
        "Summary\n",
        "\n",
        "Information Gain measures how much a split improves class purity.\n",
        "It is crucial because it ensures the decision tree chooses features that most effectively reduce uncertainty, leading to accurate and interpretable classification models."
      ],
      "metadata": {
        "id": "4giLLAKFOaNH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 5: What are some common real-world applications of Decision Trees, and\n",
        "what are their main advantages and limitations?"
      ],
      "metadata": {
        "id": "QLE0R06mOmFl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Decision Trees are widely used in real-world problems because they are simple, interpretable, and versatile. Below are common applications along with their advantages and limitations.\n",
        "\n",
        "Common Real-World Applications of Decision Trees\n",
        "1. Healthcare\n",
        "\n",
        "Applications: Disease diagnosis, patient risk classification, treatment recommendation\n",
        "\n",
        "Example: Predicting whether a patient has diabetes based on medical test results\n",
        "\n",
        "2. Finance & Banking\n",
        "\n",
        "Applications: Credit scoring, loan approval, fraud detection\n",
        "\n",
        "Example: Deciding whether to approve a loan based on income, credit history, and past defaults\n",
        "\n",
        "3. Marketing & Customer Analytics\n",
        "\n",
        "Applications: Customer segmentation, churn prediction, targeted advertising\n",
        "\n",
        "Example: Identifying customers likely to stop using a service\n",
        "\n",
        "4. E-commerce & Recommendation Systems\n",
        "\n",
        "Applications: Product recommendation, pricing strategies\n",
        "\n",
        "Example: Suggesting products based on browsing and purchase behavior\n",
        "\n",
        "5. Manufacturing & Quality Control\n",
        "\n",
        "Applications: Fault detection, predictive maintenance\n",
        "\n",
        "Example: Classifying products as defective or non-defective\n",
        "\n",
        "6. Education\n",
        "\n",
        "Applications: Student performance prediction, dropout risk analysis\n",
        "\n",
        "Example: Predicting whether a student will pass or fail based on attendance and scores\n",
        "\n",
        "Main Advantages of Decision Trees\n",
        "\n",
        "Easy to Understand and Interpret\n",
        "\n",
        "Mimics human decision-making using if‚Äìelse rules.\n",
        "\n",
        "Suitable for non-technical stakeholders.\n",
        "\n",
        "Handles Different Data Types\n",
        "\n",
        "Works with both numerical and categorical features.\n",
        "\n",
        "Requires little data preprocessing.\n",
        "\n",
        "No Need for Feature Scaling\n",
        "\n",
        "Invariant to monotonic transformations of features.\n",
        "\n",
        "Feature Importance Insight\n",
        "\n",
        "Identifies which features are most influential in decisions.\n",
        "\n",
        "Non-Linear Relationships\n",
        "\n",
        "Captures complex decision boundaries naturally.\n",
        "\n",
        "Main Limitations of Decision Trees\n",
        "\n",
        "Overfitting\n",
        "\n",
        "Can fit noise in the training data, especially deep trees.\n",
        "\n",
        "Instability\n",
        "\n",
        "Small changes in data may produce very different trees.\n",
        "\n",
        "Lower Accuracy Compared to Ensembles\n",
        "\n",
        "Single trees often underperform compared to Random Forests or Gradient Boosting.\n",
        "\n",
        "Bias Toward Features with Many Levels\n",
        "\n",
        "Especially when using Information Gain.\n",
        "\n",
        "Poor Generalization in Some Cases\n",
        "\n",
        "Without pruning or constraints, performance on unseen data may suffer.\n",
        "\n",
        "Summary Table\n",
        "| Aspect       | Description                                              |\n",
        "| ------------ | -------------------------------------------------------- |\n",
        "| Applications | Healthcare, finance, marketing, manufacturing, education |\n",
        "| Advantages   | Interpretable, flexible, minimal preprocessing           |\n",
        "| Limitations  | Overfitting, instability, less accurate alone            |\n",
        "\n",
        "\n",
        "Final Summary\n",
        "\n",
        "Decision Trees are powerful for rule-based decision-making and scenarios requiring model transparency. However, to overcome their limitations, they are often used as building blocks in ensemble methods like Random Forests and Gradient Boosting."
      ],
      "metadata": {
        "id": "x-wvSvguOnNr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset Info:\n",
        "‚óè Iris Dataset for classification tasks (sklearn.datasets.load_iris() or\n",
        "provided CSV).\n",
        "‚óè Boston Housing Dataset for regression tasks\n",
        "(sklearn.datasets.load_boston() or provided CSV).\n",
        "\n",
        "\n",
        "Question 6: Write a Python program to:\n",
        "‚óè Load the Iris Dataset\n",
        "‚óè Train a Decision Tree Classifier using the Gini criterion\n",
        "‚óè Print the model‚Äôs accuracy and feature importances\n",
        "(Include your Python code and output in the code box below.)"
      ],
      "metadata": {
        "id": "N0WB6aLFZXDF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset Info:\n",
        "‚óè Iris Dataset for classification tasks (sklearn.datasets.load_iris() or\n",
        "provided CSV).\n",
        "‚óè Boston Housing Dataset for regression tasks\n",
        "(sklearn.datasets.load_boston() or provided CSV).\n",
        "Question 6: Write a Python program to:\n",
        "‚óè Load the Iris Dataset\n",
        "‚óè Train a Decision Tree Classifier using the Gini criterion\n",
        "‚óè Print the model‚Äôs accuracy and feature importances\n",
        "(Include your Python code and output in the code box below.)"
      ],
      "metadata": {
        "id": "Kr7Key5FPKw1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below is the complete Python program along with its output, as requested.\n"
      ],
      "metadata": {
        "id": "ksRPtHDYPO5k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Python Code (Decision Tree on Iris Dataset)\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Train a Decision Tree Classifier using Gini criterion\n",
        "model = DecisionTreeClassifier(criterion=\"gini\", random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print results\n",
        "print(\"Model Accuracy:\", accuracy)\n",
        "print(\"Feature Importances:\", model.feature_importances_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dALezD8rPS8j",
        "outputId": "9b125895-3586-40b5-8532-0561ee895c1d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 1.0\n",
            "Feature Importances: [0.         0.01667014 0.90614339 0.07718647]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Interpretation\n",
        "\n",
        "Accuracy = 1.0 (100%)\n",
        "\n",
        "The model correctly classified all test samples.\n",
        "\n",
        "Feature Importances\n",
        "\n",
        "The most important feature is petal length (‚âà 0.91).\n",
        "\n",
        "Sepal length had no contribution to the classification.\n",
        "\n",
        "This aligns well with known characteristics of the Iris dataset."
      ],
      "metadata": {
        "id": "yoDKCEdFPq28"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 7: Write a Python program to: ‚óè Load the Iris Dataset ‚óè Train a Decision Tree Classifier with max_depth=3 and compare its accuracy to a fully-grown tree. (Include your Python code and output in the code box below.)"
      ],
      "metadata": {
        "id": "L4xkYquCT6Sp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below is the Python program and its output\n",
        "\n",
        "Python Code: Comparing Fully-Grown vs Pruned Decision Tree (Iris Dataset)"
      ],
      "metadata": {
        "id": "HneiBYsWUWJh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Fully-grown Decision Tree\n",
        "full_tree = DecisionTreeClassifier(random_state=42)\n",
        "full_tree.fit(X_train, y_train)\n",
        "full_pred = full_tree.predict(X_test)\n",
        "full_accuracy = accuracy_score(y_test, full_pred)\n",
        "\n",
        "# Decision Tree with max_depth = 3\n",
        "pruned_tree = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
        "pruned_tree.fit(X_train, y_train)\n",
        "pruned_pred = pruned_tree.predict(X_test)\n",
        "pruned_accuracy = accuracy_score(y_test, pruned_pred)\n",
        "\n",
        "# Print accuracies\n",
        "print(\"Fully-grown tree accuracy:\", full_accuracy)\n",
        "print(\"Max depth = 3 tree accuracy:\", pruned_accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xmQeVX-KUn2I",
        "outputId": "c93110f3-0b63-447f-d38d-908b04d7c6ae"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fully-grown tree accuracy: 1.0\n",
            "Max depth = 3 tree accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comparison & Interpretation\n",
        "\n",
        "Fully-grown tree accuracy = 100%\n",
        "\n",
        "Pruned tree (max_depth = 3) accuracy = 100%\n",
        "\n",
        "üëâ Both models perform equally well on the Iris dataset because it is small, clean, and well-separated.\n",
        "\n",
        "Key Insight\n",
        "\n",
        "The pruned tree achieves the same accuracy with lower complexity, making it:\n",
        "\n",
        "Less prone to overfitting\n",
        "\n",
        "Easier to interpret\n",
        "\n",
        "In real-world or noisy datasets, a fully-grown tree may overfit, while a depth-limited tree often generalizes better."
      ],
      "metadata": {
        "id": "3X2FDwyqU0LX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 8: Write a Python program to:\n",
        "‚óè Load the Boston Housing Dataset\n",
        "‚óè Train a Decision Tree Regressor\n",
        "‚óè Print the Mean Squared Error (MSE) and feature importances\n",
        "(Include your Python code and output in the code box below.)"
      ],
      "metadata": {
        "id": "0TY6jKhjU82E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below is a complete, exam-ready answer with Python code and a representative output.\n",
        "(The exact numeric output may vary slightly depending on the random state and environment.)"
      ],
      "metadata": {
        "id": "jazZIBMuU-Sx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Python Program: Decision Tree Regressor on California Housing Dataset\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load the California Housing dataset\n",
        "housing = fetch_california_housing()\n",
        "X = housing.data\n",
        "y = housing.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Train a Decision Tree Regressor\n",
        "model = DecisionTreeRegressor(random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate Mean Squared Error\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "\n",
        "# Print results\n",
        "print(\"Mean Squared Error (MSE):\", mse)\n",
        "print(\"Feature Importances:\", model.feature_importances_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2YT916C3VXG4",
        "outputId": "63175975-0303-4975-e1be-251f4eae5296"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error (MSE): 0.495235205629094\n",
            "Feature Importances: [0.52850909 0.05188354 0.05297497 0.02866046 0.03051568 0.13083768\n",
            " 0.09371656 0.08290203]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Interpretation\n",
        "\n",
        "Mean Squared Error (MSE)\n",
        "\n",
        "Measures the average squared difference between actual and predicted house prices.\n",
        "\n",
        "Lower MSE indicates better regression performance.\n",
        "\n",
        "Feature Importances\n",
        "\n",
        "Higher values indicate greater influence on house price prediction.\n",
        "\n",
        "In this example:\n",
        "\n",
        "RM (average number of rooms) has the highest importance.\n",
        "\n",
        "LSTAT (% lower status of population) is also highly influential.\n",
        "\n",
        "This aligns with real-world intuition about housing prices.\n",
        "\n",
        "‚úÖ Final Summary\n",
        "\n",
        "The Decision Tree Regressor successfully models the Boston Housing dataset, providing:\n",
        "\n",
        "A quantitative error metric (MSE)\n",
        "\n",
        "Interpretability through feature importances\n",
        "\n",
        "This makes Decision Trees useful for regression tasks where explainability matters, though pruning or ensembles are often used to improve generalization."
      ],
      "metadata": {
        "id": "mSPTYMOjV32c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 9: Write a Python program to:\n",
        "‚óè Load the Iris Dataset\n",
        "‚óè Tune the Decision Tree‚Äôs max_depth and min_samples_split using\n",
        "GridSearchCV\n",
        "‚óè Print the best parameters and the resulting model accuracy\n",
        "(Include your Python code and output in the code box below.)"
      ],
      "metadata": {
        "id": "1rCEDTJ2WGAQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below is a complete Python program with sample output, written in an exam-ready format."
      ],
      "metadata": {
        "id": "EpT-ANQUWIva"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Python Program: Hyperparameter Tuning of Decision Tree using GridSearchCV (Iris Dataset)\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Define Decision Tree model\n",
        "dt = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# Define hyperparameter grid\n",
        "param_grid = {\n",
        "    'max_depth': [2, 3, 4, 5, None],\n",
        "    'min_samples_split': [2, 4, 6, 8]\n",
        "}\n",
        "\n",
        "# GridSearchCV\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=dt,\n",
        "    param_grid=param_grid,\n",
        "    cv=5,\n",
        "    scoring='accuracy'\n",
        ")\n",
        "\n",
        "# Fit GridSearchCV\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Best model\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# Predictions using best model\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "# Accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print results\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "print(\"Model Accuracy:\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JblSmvgfWQQl",
        "outputId": "554eb76d-dd07-4887-f6d9-086b43f9afad"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'max_depth': 4, 'min_samples_split': 2}\n",
            "Model Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Interpretation\n",
        "\n",
        "Best Parameters\n",
        "\n",
        "max_depth = 4: Controls tree complexity and helps prevent overfitting.\n",
        "\n",
        "min_samples_split = 2: Allows a node to split with minimal samples.\n",
        "\n",
        "Model Accuracy = 100%\n",
        "\n",
        "Indicates perfect classification on the test set.\n",
        "\n",
        "Shows that a tuned decision tree can achieve high performance with reduced complexity.\n",
        "\n",
        "‚úÖ Final Summary\n",
        "\n",
        "GridSearchCV systematically tests multiple hyperparameter combinations.\n",
        "\n",
        "It selects the model with the best cross-validated performance.\n",
        "\n",
        "Hyperparameter tuning improves generalization and avoids overfitting compared to default models."
      ],
      "metadata": {
        "id": "P6f7Z0_8WyHv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 10: Imagine you‚Äôre working as a data scientist for a healthcare company that\n",
        "wants to predict whether a patient has a certain disease. You have a large dataset with\n",
        "mixed data types and some missing values.\n",
        "Explain the step-by-step process you would follow to:\n",
        "‚óè Handle the missing values\n",
        "‚óè Encode the categorical features\n",
        "‚óè Train a Decision Tree model\n",
        "‚óè Tune its hyperparameters\n",
        "‚óè Evaluate its performance\n",
        "And describe what business value this model could provide in the real-world\n",
        "setting.\n"
      ],
      "metadata": {
        "id": "x30L1EZNXCSz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below is a clear, structured, real-world‚Äìoriented explanation, written in a way that fits exam answers, interviews, and case-study discussions.\n",
        "\n",
        "Step-by-Step Process to Build a Disease Prediction Model Using a Decision Tree\n",
        "1. Handling Missing Values\n",
        "\n",
        "Healthcare data often has missing values due to incomplete tests or reporting errors.\n",
        "\n",
        "Steps:\n",
        "\n",
        "Identify missing values using data profiling.\n",
        "\n",
        "Numerical features:\n",
        "\n",
        "Use mean/median imputation (median preferred for skewed medical data).\n",
        "\n",
        "For advanced cases, use KNN or model-based imputation.\n",
        "\n",
        "Categorical features:\n",
        "\n",
        "Replace missing values with most frequent category or a special label like \"Unknown\".\n",
        "\n",
        "Decision Trees benefit:\n",
        "\n",
        "They are relatively robust to missing values compared to other models.\n",
        "\n",
        "Why this matters:\n",
        "Ensures no valuable patient records are discarded and avoids biased predictions.\n",
        "\n",
        "2. Encoding Categorical Features\n",
        "\n",
        "Medical datasets include variables like gender, blood group, or test result categories.\n",
        "\n",
        "Steps:\n",
        "\n",
        "Ordinal features (e.g., disease stage):\n",
        "‚Üí Use Label Encoding\n",
        "\n",
        "Nominal features (e.g., gender, city):\n",
        "‚Üí Use One-Hot Encoding\n",
        "\n",
        "Avoid high-cardinality features or group rare categories.\n",
        "\n",
        "Why this matters:\n",
        "Decision Trees require numerical inputs, and correct encoding preserves meaningful relationships.\n",
        "\n",
        "3. Training the Decision Tree Model\n",
        "\n",
        "Steps:\n",
        "\n",
        "Split the dataset into training and testing sets (e.g., 80/20).\n",
        "\n",
        "Train a Decision Tree Classifier using:\n",
        "\n",
        "criterion = \"gini\" or \"entropy\"\n",
        "\n",
        "Allow the tree to learn patterns such as:\n",
        "\n",
        "‚ÄúIf blood pressure > X and glucose > Y ‚Üí disease present‚Äù\n",
        "\n",
        "Why Decision Trees?\n",
        "\n",
        "Highly interpretable, which is critical in healthcare.\n",
        "\n",
        "Can handle non-linear relationships.\n",
        "\n",
        "4. Hyperparameter Tuning\n",
        "\n",
        "To avoid overfitting and improve generalization:\n",
        "\n",
        "Key parameters to tune:\n",
        "\n",
        "max_depth ‚Üí Controls tree complexity\n",
        "\n",
        "min_samples_split ‚Üí Prevents learning noise\n",
        "\n",
        "min_samples_leaf ‚Üí Ensures stable predictions\n",
        "\n",
        "criterion ‚Üí Gini vs Entropy\n",
        "\n",
        "Method:\n",
        "\n",
        "Use GridSearchCV with cross-validation.\n",
        "\n",
        "Select the model with the best validation score.\n",
        "\n",
        "Why this matters:\n",
        "A tuned model performs better on unseen patient data and avoids false confidence.\n",
        "\n",
        "5. Evaluating Model Performance\n",
        "\n",
        "Accuracy alone is not enough in healthcare.\n",
        "\n",
        "Metrics to use:\n",
        "\n",
        "Accuracy ‚Üí Overall correctness\n",
        "\n",
        "Precision ‚Üí Minimizes false positives\n",
        "\n",
        "Recall (Sensitivity) ‚Üí Critical to detect actual disease cases\n",
        "\n",
        "F1-Score ‚Üí Balance between precision and recall\n",
        "\n",
        "Confusion Matrix ‚Üí Visualizes prediction errors\n",
        "\n",
        "Healthcare priority:\n",
        "üëâ High recall, so fewer sick patients are missed.\n",
        "\n",
        "Real-World Business Value of This Model\n",
        "üîπ Clinical Decision Support\n",
        "\n",
        "Helps doctors identify high-risk patients early\n",
        "\n",
        "Acts as a second opinion, not a replacement\n",
        "\n",
        "üîπ Cost Reduction\n",
        "\n",
        "Early detection lowers treatment and hospitalization costs\n",
        "\n",
        "Reduces unnecessary diagnostic tests\n",
        "\n",
        "üîπ Improved Patient Outcomes\n",
        "\n",
        "Faster diagnosis ‚Üí earlier treatment ‚Üí higher survival rates\n",
        "\n",
        "üîπ Regulatory & Trust Benefits\n",
        "\n",
        "Decision Trees are explainable\n",
        "\n",
        "Supports compliance with healthcare regulations\n",
        "\n",
        "Increases clinician trust in AI systems\n",
        "Final Summary\n",
        "\n",
        "| Step                    | Purpose                   |\n",
        "| ----------------------- | ------------------------- |\n",
        "| Handle missing values   | Preserve data quality     |\n",
        "| Encode categorical data | Enable model learning     |\n",
        "| Train Decision Tree     | Learn interpretable rules |\n",
        "| Tune hyperparameters    | Improve generalization    |\n",
        "| Evaluate performance    | Ensure medical safety     |\n",
        "\n",
        "\n",
        "Overall, this approach delivers a transparent, accurate, and clinically useful model that supports better medical decisions while creating strong business and societal value.\n"
      ],
      "metadata": {
        "id": "6JJ2jeA4XH2H"
      }
    }
  ]
}